{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np \n",
    "import os\n",
    "import scipy.io as sio\n",
    "#import rarfile\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#from hmmlearn import hmm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "path = '/'.join(cwd.split('\\\\'))+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "test_idx = random.sample(range(1,101), 20)\n",
    "idx = list(set(range(1,101)) - set(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_idx = []\n",
    "val_idx = []\n",
    "for train_index, val_index in kf.split(idx):\n",
    "    train = np.array(idx)[train_index]\n",
    "    val = np.array(idx)[val_index]\n",
    "    train_idx.append(train)\n",
    "    val_idx.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_list, target_list):\n",
    "        assert len(data_list) == len(target_list)\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        dat = self.data_list[index]\n",
    "        label = self.target_list[index]\n",
    "        \n",
    "        return [dat, label]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newsgroup_collate_func(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[1])\n",
    "        data_list.append(datum[0])\n",
    "        \n",
    "    x = torch.from_numpy(np.array(data_list)).float().to(device)\n",
    "    y = torch.LongTensor(np.array(label_list)).to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.rnn = nn.GRU(1, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size, 3)\n",
    "\n",
    "  \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size).float()\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size).to(device)\n",
    "\n",
    "        _, self.hidden = self.rnn(x, self.hidden)\n",
    "        final_hidden = self.hidden.view(1, 1, batch_size, self.hidden_size).contiguous().view(batch_size, -1)\n",
    "        out = self.linear1(final_hidden)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(data)\n",
    "        loss = criterion(y_hat, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * len(data) / len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    y_hat_ls = []\n",
    "    y_ls = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            y_hat = model(data)\n",
    "            loss = criterion(y_hat, labels.long())\n",
    "            y_hat_ls.append(y_hat)\n",
    "            y_ls.append(labels)\n",
    "            val_loss += loss.item() * len(data) / len(dataloader.dataset)\n",
    "            \n",
    "    return val_loss, torch.cat(y_hat_ls, dim=0), torch.cat(y_ls, dim=0)\n",
    "def acc(model, dataloader, criterion):\n",
    "    val_loss, pred,true = do_eval(\n",
    "    model = model,\n",
    "    dataloader = dataloader,\n",
    "    criterion = criterion\n",
    "    )\n",
    "    return val_loss, (torch.exp(pred).max(1)[1] == true).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2\n",
      "Epoch: [1/100], Train Loss: 1.0329049046039591, Val Loss: 1.0364032821655271, Val Acc: 0.5009999871253967\n",
      "Epoch: [2/100], Train Loss: 0.8386963739395141, Val Loss: 0.8443837757110593, Val Acc: 0.6495000123977661\n",
      "Epoch: [3/100], Train Loss: 0.7969148950576782, Val Loss: 0.8089122128486631, Val Acc: 0.6622499823570251\n",
      "Epoch: [4/100], Train Loss: 0.7661393222808839, Val Loss: 0.7771106975078583, Val Acc: 0.6697499752044678\n",
      "Epoch: [5/100], Train Loss: 0.7325460927486417, Val Loss: 0.7544301345348357, Val Acc: 0.6837499737739563\n",
      "Epoch: [6/100], Train Loss: 0.7085424705743781, Val Loss: 0.7189963569641111, Val Acc: 0.7067499756813049\n",
      "Epoch: [7/100], Train Loss: 0.6848348517417907, Val Loss: 0.7058711342811586, Val Acc: 0.7110000252723694\n",
      "Epoch: [8/100], Train Loss: 0.6538910772800444, Val Loss: 0.671654185771942, Val Acc: 0.7287499904632568\n",
      "Epoch: [9/100], Train Loss: 0.6346396754980085, Val Loss: 0.6536511774063112, Val Acc: 0.7379999756813049\n",
      "Epoch: [10/100], Train Loss: 0.6236554404497145, Val Loss: 0.6465727646350863, Val Acc: 0.7397500276565552\n",
      "Epoch: [11/100], Train Loss: 0.6109519492387774, Val Loss: 0.6190208048820494, Val Acc: 0.7437499761581421\n",
      "Epoch: [12/100], Train Loss: 0.5880324867963791, Val Loss: 0.6025369760990142, Val Acc: 0.7555000185966492\n",
      "Epoch: [13/100], Train Loss: 0.5832131907939911, Val Loss: 0.6105932307243349, Val Acc: 0.7507500052452087\n",
      "Epoch: [14/100], Train Loss: 0.5565784153938298, Val Loss: 0.5771648440361024, Val Acc: 0.7664999961853027\n",
      "Epoch: [15/100], Train Loss: 0.5460357457399367, Val Loss: 0.5654538064002987, Val Acc: 0.7730000019073486\n",
      "Epoch: [16/100], Train Loss: 0.5329170655012131, Val Loss: 0.5501488089561462, Val Acc: 0.7754999995231628\n",
      "Epoch: [17/100], Train Loss: 0.5384070168733599, Val Loss: 0.5469020800590514, Val Acc: 0.784500002861023\n",
      "Epoch: [18/100], Train Loss: 0.51216798889637, Val Loss: 0.5364817492961884, Val Acc: 0.7802500128746033\n",
      "Epoch: [19/100], Train Loss: 0.50750339615345, Val Loss: 0.533452587246895, Val Acc: 0.7837499976158142\n",
      "Epoch: [20/100], Train Loss: 0.5172968066930769, Val Loss: 0.5525008974075317, Val Acc: 0.7797499895095825\n",
      "Epoch: [21/100], Train Loss: 0.49009822881221776, Val Loss: 0.5220662229061126, Val Acc: 0.7917500138282776\n",
      "Epoch: [22/100], Train Loss: 0.4835524966716767, Val Loss: 0.5053829686641693, Val Acc: 0.8027499914169312\n",
      "Epoch: [23/100], Train Loss: 0.4779714944362643, Val Loss: 0.5128979904651642, Val Acc: 0.7954999804496765\n",
      "Epoch: [24/100], Train Loss: 0.49252697741985324, Val Loss: 0.5388373649120329, Val Acc: 0.7839999794960022\n",
      "Epoch: [25/100], Train Loss: 0.46812349861860275, Val Loss: 0.49160826790332784, Val Acc: 0.8050000071525574\n",
      "Epoch: [26/100], Train Loss: 0.4632854853868485, Val Loss: 0.4845899275541305, Val Acc: 0.8107500076293945\n",
      "Epoch: [27/100], Train Loss: 0.4464274688959123, Val Loss: 0.4946387314796449, Val Acc: 0.8075000047683716\n",
      "Epoch: [28/100], Train Loss: 0.4382162566781043, Val Loss: 0.47245872521400445, Val Acc: 0.8165000081062317\n",
      "Epoch: [29/100], Train Loss: 0.4364587777256966, Val Loss: 0.4781012420654297, Val Acc: 0.8102499842643738\n",
      "Epoch: [30/100], Train Loss: 0.44306779676675795, Val Loss: 0.47347391802072525, Val Acc: 0.8130000233650208\n",
      "Epoch: [31/100], Train Loss: 0.433934756696224, Val Loss: 0.4931777697205542, Val Acc: 0.8040000200271606\n",
      "Epoch: [32/100], Train Loss: 0.4234157723784447, Val Loss: 0.48552206808328646, Val Acc: 0.8075000047683716\n",
      "Epoch: [33/100], Train Loss: 0.4087661995291711, Val Loss: 0.45951642560958866, Val Acc: 0.824999988079071\n",
      "Epoch: [34/100], Train Loss: 0.40101181221008325, Val Loss: 0.4557293272614479, Val Acc: 0.8257499933242798\n",
      "Epoch: [35/100], Train Loss: 0.3697161056995394, Val Loss: 0.4177168626785277, Val Acc: 0.8420000076293945\n",
      "Epoch: [36/100], Train Loss: 0.35079378837347064, Val Loss: 0.40528084075450904, Val Acc: 0.846750020980835\n",
      "Epoch: [37/100], Train Loss: 0.3619328783750534, Val Loss: 0.4058240306377411, Val Acc: 0.8472499847412109\n",
      "Epoch: [38/100], Train Loss: 0.344975661933422, Val Loss: 0.39907508087158183, Val Acc: 0.8535000085830688\n",
      "Epoch: [39/100], Train Loss: 0.33554525691270837, Val Loss: 0.39751175326108923, Val Acc: 0.8569999933242798\n",
      "Epoch: [40/100], Train Loss: 0.34634638595581074, Val Loss: 0.39658896845579156, Val Acc: 0.8552500009536743\n",
      "Epoch: [41/100], Train Loss: 0.3785869098305701, Val Loss: 0.4442973791360854, Val Acc: 0.8347499966621399\n",
      "Epoch: [42/100], Train Loss: 0.3292252514362335, Val Loss: 0.39231248342990876, Val Acc: 0.8517500162124634\n",
      "Epoch: [43/100], Train Loss: 0.323779497563839, Val Loss: 0.38479614573717114, Val Acc: 0.8569999933242798\n",
      "Epoch: [44/100], Train Loss: 0.3204297107458115, Val Loss: 0.3883250775933265, Val Acc: 0.8572499752044678\n",
      "Epoch: [45/100], Train Loss: 0.31785053884983056, Val Loss: 0.3855107676684855, Val Acc: 0.859000027179718\n",
      "Epoch: [46/100], Train Loss: 0.3148762053847313, Val Loss: 0.37822027629613875, Val Acc: 0.859499990940094\n",
      "Epoch: [47/100], Train Loss: 0.3074673750996589, Val Loss: 0.38187399780750286, Val Acc: 0.8615000247955322\n",
      "Epoch: [48/100], Train Loss: 0.30888564324378964, Val Loss: 0.3954412855505943, Val Acc: 0.859749972820282\n",
      "Epoch: [49/100], Train Loss: 0.31438557356596003, Val Loss: 0.39497469660639756, Val Acc: 0.8550000190734863\n",
      "Epoch: [50/100], Train Loss: 0.30463342940807314, Val Loss: 0.39161376333236686, Val Acc: 0.8560000061988831\n",
      "Epoch: [51/100], Train Loss: 0.30664986407756795, Val Loss: 0.3900622746944427, Val Acc: 0.8579999804496765\n",
      "Epoch: [52/100], Train Loss: 0.2990260533094407, Val Loss: 0.3807350675165653, Val Acc: 0.8612499833106995\n",
      "Epoch: [53/100], Train Loss: 0.29280919069051775, Val Loss: 0.38209608125686656, Val Acc: 0.8647500276565552\n",
      "Epoch: [54/100], Train Loss: 0.29845701694488547, Val Loss: 0.38939967644214635, Val Acc: 0.8612499833106995\n",
      "Epoch: [55/100], Train Loss: 0.2990178575515748, Val Loss: 0.38531506240367897, Val Acc: 0.8617500066757202\n",
      "Epoch: [56/100], Train Loss: 0.3010149209201337, Val Loss: 0.37051905336976054, Val Acc: 0.862500011920929\n",
      "Epoch: [57/100], Train Loss: 0.3003104315698147, Val Loss: 0.38398541915416723, Val Acc: 0.8612499833106995\n",
      "Epoch: [58/100], Train Loss: 0.2927857475876806, Val Loss: 0.3838304498195649, Val Acc: 0.8640000224113464\n",
      "Epoch: [59/100], Train Loss: 0.28360243782401107, Val Loss: 0.3741724451184273, Val Acc: 0.8627499938011169\n",
      "Epoch: [60/100], Train Loss: 0.28695723775029186, Val Loss: 0.37742757397890103, Val Acc: 0.8659999966621399\n",
      "Epoch: [61/100], Train Loss: 0.28278844809532155, Val Loss: 0.3750102730095387, Val Acc: 0.8700000047683716\n",
      "Epoch: [62/100], Train Loss: 0.28637485265731805, Val Loss: 0.3848109240233898, Val Acc: 0.8650000095367432\n",
      "Epoch: [63/100], Train Loss: 0.2781765773296356, Val Loss: 0.36993721318244943, Val Acc: 0.8632500171661377\n",
      "Epoch: [64/100], Train Loss: 0.2783911876082423, Val Loss: 0.3665713049769401, Val Acc: 0.8692499995231628\n",
      "Epoch: [65/100], Train Loss: 0.27021965527534486, Val Loss: 0.3611753525137901, Val Acc: 0.8715000152587891\n",
      "Epoch: [66/100], Train Loss: 0.2787605190575122, Val Loss: 0.39617645624279973, Val Acc: 0.8607500195503235\n",
      "Epoch: [67/100], Train Loss: 0.2701569932401178, Val Loss: 0.3717878035902977, Val Acc: 0.8692499995231628\n",
      "Epoch: [68/100], Train Loss: 0.26541291272640233, Val Loss: 0.3668195891976356, Val Acc: 0.8677499890327454\n",
      "Epoch: [69/100], Train Loss: 0.28879151558876004, Val Loss: 0.38175889584422107, Val Acc: 0.8617500066757202\n",
      "Epoch: [70/100], Train Loss: 0.270807162851095, Val Loss: 0.37431429925560955, Val Acc: 0.8665000200271606\n",
      "Epoch: [71/100], Train Loss: 0.27190210163593287, Val Loss: 0.3873985061049462, Val Acc: 0.8662499785423279\n",
      "Epoch: [72/100], Train Loss: 0.2597553516030311, Val Loss: 0.35184661996364597, Val Acc: 0.8772500157356262\n",
      "Epoch: [73/100], Train Loss: 0.25627557286620123, Val Loss: 0.34887611570954324, Val Acc: 0.8757500052452087\n",
      "Epoch: [74/100], Train Loss: 0.25932180339097965, Val Loss: 0.35936777102947226, Val Acc: 0.8694999814033508\n",
      "Epoch: [75/100], Train Loss: 0.2835264220833778, Val Loss: 0.38214388751983636, Val Acc: 0.8579999804496765\n",
      "Epoch: [76/100], Train Loss: 0.27146092888712886, Val Loss: 0.37506118294596674, Val Acc: 0.862500011920929\n",
      "Epoch: [77/100], Train Loss: 0.2472147664129733, Val Loss: 0.3648614282906056, Val Acc: 0.8702499866485596\n",
      "Epoch: [78/100], Train Loss: 0.2530610072612761, Val Loss: 0.365902858734131, Val Acc: 0.8727499842643738\n",
      "Epoch: [79/100], Train Loss: 0.2558815901577473, Val Loss: 0.37674089196324334, Val Acc: 0.8640000224113464\n",
      "Epoch: [80/100], Train Loss: 0.2505434918403625, Val Loss: 0.3667714694142342, Val Acc: 0.8709999918937683\n",
      "Epoch: [81/100], Train Loss: 0.24855540713667873, Val Loss: 0.366072697877884, Val Acc: 0.8675000071525574\n",
      "Epoch: [82/100], Train Loss: 0.2366554742157459, Val Loss: 0.35378812247514724, Val Acc: 0.8792499899864197\n",
      "Epoch: [83/100], Train Loss: 0.2541196319758892, Val Loss: 0.36958258306980124, Val Acc: 0.8629999756813049\n",
      "Epoch: [84/100], Train Loss: 0.23540202683210365, Val Loss: 0.36602965247631064, Val Acc: 0.8727499842643738\n",
      "Epoch: [85/100], Train Loss: 0.22976329499483106, Val Loss: 0.35661479991674416, Val Acc: 0.8767499923706055\n",
      "Epoch: [86/100], Train Loss: 0.23489201200008394, Val Loss: 0.3618715374171733, Val Acc: 0.871749997138977\n",
      "Epoch: [87/100], Train Loss: 0.23475579234957694, Val Loss: 0.355278225004673, Val Acc: 0.8727499842643738\n",
      "Epoch: [88/100], Train Loss: 0.22445185908675191, Val Loss: 0.3555074348151684, Val Acc: 0.8774999976158142\n",
      "Epoch: [89/100], Train Loss: 0.2440773824155331, Val Loss: 0.3831720860898495, Val Acc: 0.8727499842643738\n",
      "Epoch: [90/100], Train Loss: 0.24195677459239961, Val Loss: 0.36247412508726123, Val Acc: 0.871999979019165\n",
      "Epoch: [91/100], Train Loss: 0.22132560309767724, Val Loss: 0.3501326069533825, Val Acc: 0.8767499923706055\n",
      "Epoch: [92/100], Train Loss: 0.22621130201220518, Val Loss: 0.34387878388166426, Val Acc: 0.8737499713897705\n",
      "Epoch: [93/100], Train Loss: 0.22496949273347855, Val Loss: 0.3520013422667981, Val Acc: 0.8767499923706055\n",
      "Epoch: [94/100], Train Loss: 0.22461824306845654, Val Loss: 0.35966634148359294, Val Acc: 0.875249981880188\n",
      "Epoch: [95/100], Train Loss: 0.22022775015234952, Val Loss: 0.3444240768253804, Val Acc: 0.8769999742507935\n",
      "Epoch: [96/100], Train Loss: 0.22346077230572692, Val Loss: 0.3663072041273117, Val Acc: 0.8772500157356262\n",
      "Epoch: [97/100], Train Loss: 0.2137237592637539, Val Loss: 0.3368629276156425, Val Acc: 0.8805000185966492\n",
      "Epoch: [98/100], Train Loss: 0.21329472780227657, Val Loss: 0.34475176835060106, Val Acc: 0.8790000081062317\n",
      "Epoch: [99/100], Train Loss: 0.21242905455827707, Val Loss: 0.33817289096117026, Val Acc: 0.8794999718666077\n",
      "Epoch: [100/100], Train Loss: 0.21811942975223064, Val Loss: 0.33272736215591425, Val Acc: 0.878250002861023\n",
      "k = 3\n",
      "Epoch: [1/100], Train Loss: 1.044757751464843, Val Loss: 1.0463980693817139, Val Acc: 0.45399999618530273\n",
      "Epoch: [2/100], Train Loss: 0.8238412785530094, Val Loss: 0.9233233776092532, Val Acc: 0.6140000224113464\n",
      "Epoch: [3/100], Train Loss: 0.8031638023853302, Val Loss: 0.8732628364562987, Val Acc: 0.6234999895095825\n",
      "Epoch: [4/100], Train Loss: 0.775541526317597, Val Loss: 0.8319900717735291, Val Acc: 0.640999972820282\n",
      "Epoch: [5/100], Train Loss: 0.7617589442729951, Val Loss: 0.8370479514598846, Val Acc: 0.6370000243186951\n",
      "Epoch: [6/100], Train Loss: 0.7368914310932163, Val Loss: 0.7953761734962463, Val Acc: 0.6467499732971191\n",
      "Epoch: [7/100], Train Loss: 0.7026344548463819, Val Loss: 0.7576272234916684, Val Acc: 0.671750009059906\n",
      "Epoch: [8/100], Train Loss: 0.6763861545324329, Val Loss: 0.7146048600673677, Val Acc: 0.6834999918937683\n",
      "Epoch: [9/100], Train Loss: 0.6539126040935518, Val Loss: 0.6655099513530732, Val Acc: 0.7245000004768372\n",
      "Epoch: [10/100], Train Loss: 0.6319586085081101, Val Loss: 0.6397952284812928, Val Acc: 0.7302500009536743\n",
      "Epoch: [11/100], Train Loss: 0.6252425411939618, Val Loss: 0.6759452397823335, Val Acc: 0.7129999995231628\n",
      "Epoch: [12/100], Train Loss: 0.5942136876583096, Val Loss: 0.624121598958969, Val Acc: 0.7397500276565552\n",
      "Epoch: [13/100], Train Loss: 0.5800070885419845, Val Loss: 0.6378142669200898, Val Acc: 0.7432500123977661\n",
      "Epoch: [14/100], Train Loss: 0.5531860417127611, Val Loss: 0.6016749131679535, Val Acc: 0.7494999766349792\n",
      "Epoch: [15/100], Train Loss: 0.5566609940528875, Val Loss: 0.6271466407775879, Val Acc: 0.7400000095367432\n",
      "Epoch: [16/100], Train Loss: 0.5352467998266219, Val Loss: 0.6149136869907379, Val Acc: 0.7457500100135803\n",
      "Epoch: [17/100], Train Loss: 0.5272858024835586, Val Loss: 0.6083309071063995, Val Acc: 0.7369999885559082\n",
      "Epoch: [18/100], Train Loss: 0.5073195006847381, Val Loss: 0.5702897295951844, Val Acc: 0.7595000267028809\n",
      "Epoch: [19/100], Train Loss: 0.512177649974823, Val Loss: 0.5946049058437347, Val Acc: 0.7407500147819519\n",
      "Epoch: [20/100], Train Loss: 0.49487917411327365, Val Loss: 0.5626436353921891, Val Acc: 0.7697499990463257\n",
      "Epoch: [21/100], Train Loss: 0.48694336819648726, Val Loss: 0.5682083594799042, Val Acc: 0.7599999904632568\n",
      "Epoch: [22/100], Train Loss: 0.4796496180295944, Val Loss: 0.553981395483017, Val Acc: 0.7735000252723694\n",
      "Epoch: [23/100], Train Loss: 0.47038352549076073, Val Loss: 0.5712562444210053, Val Acc: 0.753000020980835\n",
      "Epoch: [24/100], Train Loss: 0.4648697135448457, Val Loss: 0.547246177792549, Val Acc: 0.7702500224113464\n",
      "Epoch: [25/100], Train Loss: 0.4671083248257633, Val Loss: 0.5380393879413604, Val Acc: 0.7857499718666077\n",
      "Epoch: [26/100], Train Loss: 0.4767957257628445, Val Loss: 0.5665021733045579, Val Acc: 0.7732499837875366\n",
      "Epoch: [27/100], Train Loss: 0.44932109713554363, Val Loss: 0.5415723202228545, Val Acc: 0.7792500257492065\n",
      "Epoch: [28/100], Train Loss: 0.44009566491842295, Val Loss: 0.5333865634202957, Val Acc: 0.7882500290870667\n",
      "Epoch: [29/100], Train Loss: 0.4343172702193257, Val Loss: 0.5391778465509415, Val Acc: 0.7804999947547913\n",
      "Epoch: [30/100], Train Loss: 0.43315413993597035, Val Loss: 0.5533886802196503, Val Acc: 0.7664999961853027\n",
      "Epoch: [31/100], Train Loss: 0.4235429173707964, Val Loss: 0.536234618782997, Val Acc: 0.7820000052452087\n",
      "Epoch: [32/100], Train Loss: 0.420026920199394, Val Loss: 0.529788409769535, Val Acc: 0.7752500176429749\n",
      "Epoch: [33/100], Train Loss: 0.418517587006092, Val Loss: 0.5266165776252747, Val Acc: 0.7882500290870667\n",
      "Epoch: [34/100], Train Loss: 0.4021883903741835, Val Loss: 0.49227091312408455, Val Acc: 0.8084999918937683\n",
      "Epoch: [35/100], Train Loss: 0.3885221853852269, Val Loss: 0.5073555932044984, Val Acc: 0.7889999747276306\n",
      "Epoch: [36/100], Train Loss: 0.3873585790991781, Val Loss: 0.5050935759544372, Val Acc: 0.7917500138282776\n",
      "Epoch: [37/100], Train Loss: 0.380936186313629, Val Loss: 0.45347229164838776, Val Acc: 0.8199999928474426\n",
      "Epoch: [38/100], Train Loss: 0.35609661167860057, Val Loss: 0.44793961673974986, Val Acc: 0.8197500109672546\n",
      "Epoch: [39/100], Train Loss: 0.3456193464994428, Val Loss: 0.41842037057876597, Val Acc: 0.8322499990463257\n",
      "Epoch: [40/100], Train Loss: 0.3509049831032753, Val Loss: 0.4354559302330017, Val Acc: 0.8222500085830688\n",
      "Epoch: [41/100], Train Loss: 0.3277866823077199, Val Loss: 0.41801871722936623, Val Acc: 0.828499972820282\n",
      "Epoch: [42/100], Train Loss: 0.330827387213707, Val Loss: 0.4283445658683777, Val Acc: 0.8142499923706055\n",
      "Epoch: [43/100], Train Loss: 0.35307423096895185, Val Loss: 0.4055586574673653, Val Acc: 0.8387500047683716\n",
      "Epoch: [44/100], Train Loss: 0.32080743801593775, Val Loss: 0.4152735942304135, Val Acc: 0.8270000219345093\n",
      "Epoch: [45/100], Train Loss: 0.31210373783111567, Val Loss: 0.394023366689682, Val Acc: 0.8432499766349792\n",
      "Epoch: [46/100], Train Loss: 0.33208969825506196, Val Loss: 0.4240267868041993, Val Acc: 0.8314999938011169\n",
      "Epoch: [47/100], Train Loss: 0.306719235897064, Val Loss: 0.3898730180859567, Val Acc: 0.8412500023841858\n",
      "Epoch: [48/100], Train Loss: 0.30419668495655094, Val Loss: 0.3809708266854286, Val Acc: 0.8462499976158142\n",
      "Epoch: [49/100], Train Loss: 0.3030075568556785, Val Loss: 0.40983916196227077, Val Acc: 0.8289999961853027\n",
      "Epoch: [50/100], Train Loss: 0.30166965508461, Val Loss: 0.40713144552707675, Val Acc: 0.8342499732971191\n",
      "Epoch: [51/100], Train Loss: 0.2972272454202173, Val Loss: 0.3815622392296789, Val Acc: 0.8475000262260437\n",
      "Epoch: [52/100], Train Loss: 0.2984968901276588, Val Loss: 0.3822359938323498, Val Acc: 0.8429999947547913\n",
      "Epoch: [53/100], Train Loss: 0.3072576841711996, Val Loss: 0.366844741165638, Val Acc: 0.8532500267028809\n",
      "Epoch: [54/100], Train Loss: 0.2920022263526912, Val Loss: 0.3618123644590377, Val Acc: 0.8529999852180481\n",
      "Epoch: [55/100], Train Loss: 0.30338697499036793, Val Loss: 0.3708838102221487, Val Acc: 0.8535000085830688\n",
      "Epoch: [56/100], Train Loss: 0.3161560196280479, Val Loss: 0.39899061000347147, Val Acc: 0.8475000262260437\n",
      "Epoch: [57/100], Train Loss: 0.2914482094645499, Val Loss: 0.38925778284668927, Val Acc: 0.8389999866485596\n",
      "Epoch: [58/100], Train Loss: 0.28061166328191767, Val Loss: 0.3719796811640263, Val Acc: 0.8537499904632568\n",
      "Epoch: [59/100], Train Loss: 0.28110857558250435, Val Loss: 0.37993677052855496, Val Acc: 0.8417500257492065\n",
      "Epoch: [60/100], Train Loss: 0.28174643543362593, Val Loss: 0.37890340760350233, Val Acc: 0.8389999866485596\n",
      "Epoch: [61/100], Train Loss: 0.2905412946939469, Val Loss: 0.38496944123506566, Val Acc: 0.84375\n",
      "Epoch: [62/100], Train Loss: 0.2805789042711255, Val Loss: 0.3847057877480984, Val Acc: 0.8412500023841858\n",
      "Epoch: [63/100], Train Loss: 0.28050281381607056, Val Loss: 0.3663585663437843, Val Acc: 0.8552500009536743\n",
      "Epoch: [64/100], Train Loss: 0.2768514625430106, Val Loss: 0.36978184098005284, Val Acc: 0.8482499718666077\n",
      "Epoch: [65/100], Train Loss: 0.2817368199825287, Val Loss: 0.402042109966278, Val Acc: 0.831250011920929\n",
      "Epoch: [66/100], Train Loss: 0.2733517647981643, Val Loss: 0.3543136130869388, Val Acc: 0.8575000166893005\n",
      "Epoch: [67/100], Train Loss: 0.27329002499580385, Val Loss: 0.35788635489344595, Val Acc: 0.8517500162124634\n",
      "Epoch: [68/100], Train Loss: 0.2664715053737166, Val Loss: 0.37428131607174875, Val Acc: 0.8495000004768372\n",
      "Epoch: [69/100], Train Loss: 0.286273120433092, Val Loss: 0.3734697663486004, Val Acc: 0.8512499928474426\n",
      "Epoch: [70/100], Train Loss: 0.2657976798117162, Val Loss: 0.36153683814406395, Val Acc: 0.8557500243186951\n",
      "Epoch: [71/100], Train Loss: 0.261186974167824, Val Loss: 0.35517086094617856, Val Acc: 0.8602499961853027\n",
      "Epoch: [72/100], Train Loss: 0.27811704608798027, Val Loss: 0.4002998051941396, Val Acc: 0.8364999890327454\n",
      "Epoch: [73/100], Train Loss: 0.2668277382254601, Val Loss: 0.35739545488357544, Val Acc: 0.8600000143051147\n",
      "Epoch: [74/100], Train Loss: 0.2697196940481663, Val Loss: 0.3592881585359573, Val Acc: 0.8582500219345093\n",
      "Epoch: [75/100], Train Loss: 0.2570534536540509, Val Loss: 0.35402584615349775, Val Acc: 0.8575000166893005\n",
      "Epoch: [76/100], Train Loss: 0.27131585240364076, Val Loss: 0.4177319641113282, Val Acc: 0.8224999904632568\n",
      "Epoch: [77/100], Train Loss: 0.2592342768311501, Val Loss: 0.3738386068344116, Val Acc: 0.8479999899864197\n",
      "Epoch: [78/100], Train Loss: 0.2635557315945627, Val Loss: 0.4109820466339589, Val Acc: 0.8289999961853027\n",
      "Epoch: [79/100], Train Loss: 0.25072191962599766, Val Loss: 0.36025826665759086, Val Acc: 0.8537499904632568\n",
      "Epoch: [80/100], Train Loss: 0.2670179228782655, Val Loss: 0.3771891916990281, Val Acc: 0.8450000286102295\n",
      "Epoch: [81/100], Train Loss: 0.26930237367749194, Val Loss: 0.3846163995265961, Val Acc: 0.843500018119812\n",
      "Epoch: [82/100], Train Loss: 0.2618631080389023, Val Loss: 0.3567482569813729, Val Acc: 0.8610000014305115\n",
      "Epoch: [83/100], Train Loss: 0.24977943229675295, Val Loss: 0.3576430819034577, Val Acc: 0.8587499856948853\n",
      "Epoch: [84/100], Train Loss: 0.25378570094704633, Val Loss: 0.3560842342376709, Val Acc: 0.859000027179718\n",
      "Epoch: [85/100], Train Loss: 0.24321806758642184, Val Loss: 0.3538448979556561, Val Acc: 0.8587499856948853\n",
      "Epoch: [86/100], Train Loss: 0.24639627620577803, Val Loss: 0.3551583803594112, Val Acc: 0.859250009059906\n",
      "Epoch: [87/100], Train Loss: 0.24295152965188035, Val Loss: 0.35573172873258596, Val Acc: 0.8557500243186951\n",
      "Epoch: [88/100], Train Loss: 0.23849379023909562, Val Loss: 0.37147795319557186, Val Acc: 0.8512499928474426\n",
      "Epoch: [89/100], Train Loss: 0.2374621513187884, Val Loss: 0.3537712678313256, Val Acc: 0.8642500042915344\n",
      "Epoch: [90/100], Train Loss: 0.2418530102074147, Val Loss: 0.3663836338818073, Val Acc: 0.847000002861023\n",
      "Epoch: [91/100], Train Loss: 0.2383477591276169, Val Loss: 0.3469825684726238, Val Acc: 0.8647500276565552\n",
      "Epoch: [92/100], Train Loss: 0.23105298084020617, Val Loss: 0.3480631776452066, Val Acc: 0.8615000247955322\n",
      "Epoch: [93/100], Train Loss: 0.23384883540868762, Val Loss: 0.3655807140469551, Val Acc: 0.8527500033378601\n",
      "Epoch: [94/100], Train Loss: 0.23276646217703817, Val Loss: 0.36164833727478973, Val Acc: 0.8535000085830688\n",
      "Epoch: [95/100], Train Loss: 0.2348738604336975, Val Loss: 0.34681086486577983, Val Acc: 0.8640000224113464\n",
      "Epoch: [96/100], Train Loss: 0.22638171179592612, Val Loss: 0.3557487411499024, Val Acc: 0.859499990940094\n",
      "Epoch: [97/100], Train Loss: 0.236109855145216, Val Loss: 0.34676449775695795, Val Acc: 0.8659999966621399\n",
      "Epoch: [98/100], Train Loss: 0.2275869843661786, Val Loss: 0.3468345452845096, Val Acc: 0.8650000095367432\n",
      "Epoch: [99/100], Train Loss: 0.22495379424095152, Val Loss: 0.35927884003520005, Val Acc: 0.862500011920929\n",
      "Epoch: [100/100], Train Loss: 0.22648304322361956, Val Loss: 0.3477732503712177, Val Acc: 0.8617500066757202\n",
      "k = 4\n",
      "Epoch: [1/100], Train Loss: 1.0310644462108607, Val Loss: 1.034702466011047, Val Acc: 0.4505000114440918\n",
      "Epoch: [2/100], Train Loss: 0.8421228771209718, Val Loss: 0.8212212224006653, Val Acc: 0.659250020980835\n",
      "Epoch: [3/100], Train Loss: 0.8226119227409364, Val Loss: 0.8236065802574157, Val Acc: 0.6497499942779541\n",
      "Epoch: [4/100], Train Loss: 0.7860919270515438, Val Loss: 0.7727494807243347, Val Acc: 0.6690000295639038\n",
      "Epoch: [5/100], Train Loss: 0.7692274684906003, Val Loss: 0.7659580297470092, Val Acc: 0.6775000095367432\n",
      "Epoch: [6/100], Train Loss: 0.7558856899738308, Val Loss: 0.7599842414855957, Val Acc: 0.6744999885559082\n",
      "Epoch: [7/100], Train Loss: 0.7354156875610356, Val Loss: 0.7137178976535797, Val Acc: 0.6959999799728394\n",
      "Epoch: [8/100], Train Loss: 0.6993988715410234, Val Loss: 0.6971451303958894, Val Acc: 0.7237499952316284\n",
      "Epoch: [9/100], Train Loss: 0.6638626397848131, Val Loss: 0.6957394392490386, Val Acc: 0.722000002861023\n",
      "Epoch: [10/100], Train Loss: 0.63756282889843, Val Loss: 0.6802296047210693, Val Acc: 0.7354999780654907\n",
      "Epoch: [11/100], Train Loss: 0.6239545582532884, Val Loss: 0.6738840353488923, Val Acc: 0.7289999723434448\n",
      "Epoch: [12/100], Train Loss: 0.6059482443332675, Val Loss: 0.6285656113624573, Val Acc: 0.7557500004768372\n",
      "Epoch: [13/100], Train Loss: 0.5876045356988909, Val Loss: 0.6205353026390075, Val Acc: 0.7559999823570251\n",
      "Epoch: [14/100], Train Loss: 0.5709356832504271, Val Loss: 0.6044706268310549, Val Acc: 0.7672500014305115\n",
      "Epoch: [15/100], Train Loss: 0.5607670096158985, Val Loss: 0.598330136537552, Val Acc: 0.7722499966621399\n",
      "Epoch: [16/100], Train Loss: 0.5388052511215208, Val Loss: 0.5934872667789457, Val Acc: 0.7760000228881836\n",
      "Epoch: [17/100], Train Loss: 0.5358403258323668, Val Loss: 0.5883360615968704, Val Acc: 0.7732499837875366\n",
      "Epoch: [18/100], Train Loss: 0.5265417906045912, Val Loss: 0.5599711856842041, Val Acc: 0.7835000157356262\n",
      "Epoch: [19/100], Train Loss: 0.5103544833660124, Val Loss: 0.5592895004749299, Val Acc: 0.7897499799728394\n",
      "Epoch: [20/100], Train Loss: 0.5041408406496051, Val Loss: 0.5486846699714661, Val Acc: 0.7857499718666077\n",
      "Epoch: [21/100], Train Loss: 0.5154693846702575, Val Loss: 0.5520240569114683, Val Acc: 0.7877500057220459\n",
      "Epoch: [22/100], Train Loss: 0.4876879441738128, Val Loss: 0.5152136247158051, Val Acc: 0.8080000281333923\n",
      "Epoch: [23/100], Train Loss: 0.48472916638851177, Val Loss: 0.5448704016208649, Val Acc: 0.7960000038146973\n",
      "Epoch: [24/100], Train Loss: 0.4816021887063977, Val Loss: 0.5287529879808426, Val Acc: 0.7987499833106995\n",
      "Epoch: [25/100], Train Loss: 0.47788199329376224, Val Loss: 0.5412563178539277, Val Acc: 0.7982500195503235\n",
      "Epoch: [26/100], Train Loss: 0.4717440021038054, Val Loss: 0.5216876118183137, Val Acc: 0.8007500171661377\n",
      "Epoch: [27/100], Train Loss: 0.4822591255307197, Val Loss: 0.5296433830261229, Val Acc: 0.7992500066757202\n",
      "Epoch: [28/100], Train Loss: 0.46731267392635334, Val Loss: 0.5156574511528014, Val Acc: 0.7985000014305115\n",
      "Epoch: [29/100], Train Loss: 0.4590115664601327, Val Loss: 0.5186276121139527, Val Acc: 0.7992500066757202\n",
      "Epoch: [30/100], Train Loss: 0.454609139561653, Val Loss: 0.48530971813201906, Val Acc: 0.812749981880188\n",
      "Epoch: [31/100], Train Loss: 0.45196384751796725, Val Loss: 0.48820383453369154, Val Acc: 0.8115000128746033\n",
      "Epoch: [32/100], Train Loss: 0.4473356099128726, Val Loss: 0.48028169000148785, Val Acc: 0.812749981880188\n",
      "Epoch: [33/100], Train Loss: 0.4345369155406952, Val Loss: 0.4928504521846771, Val Acc: 0.8090000152587891\n",
      "Epoch: [34/100], Train Loss: 0.4362435772418977, Val Loss: 0.49369890856742865, Val Acc: 0.8147500157356262\n",
      "Epoch: [35/100], Train Loss: 0.44214424586296086, Val Loss: 0.5161648582220077, Val Acc: 0.8037499785423279\n",
      "Epoch: [36/100], Train Loss: 0.4217223035693168, Val Loss: 0.47333049058914184, Val Acc: 0.8195000290870667\n",
      "Epoch: [37/100], Train Loss: 0.42682905930280673, Val Loss: 0.4634353113174438, Val Acc: 0.8212500214576721\n",
      "Epoch: [38/100], Train Loss: 0.4173698693513869, Val Loss: 0.4813242808580399, Val Acc: 0.8144999742507935\n",
      "Epoch: [39/100], Train Loss: 0.41163143986463546, Val Loss: 0.47867284297943113, Val Acc: 0.8212500214576721\n",
      "Epoch: [40/100], Train Loss: 0.4035243206024169, Val Loss: 0.4710165747404098, Val Acc: 0.8174999952316284\n",
      "Epoch: [41/100], Train Loss: 0.3966459750533105, Val Loss: 0.4740760791301727, Val Acc: 0.8255000114440918\n",
      "Epoch: [42/100], Train Loss: 0.40514290797710406, Val Loss: 0.47498794716596604, Val Acc: 0.8187500238418579\n",
      "Epoch: [43/100], Train Loss: 0.375098827660084, Val Loss: 0.44526447719335555, Val Acc: 0.8252500295639038\n",
      "Epoch: [44/100], Train Loss: 0.38298952996730823, Val Loss: 0.4010322892069815, Val Acc: 0.8387500047683716\n",
      "Epoch: [45/100], Train Loss: 0.37154130536317787, Val Loss: 0.4090231052637101, Val Acc: 0.8370000123977661\n",
      "Epoch: [46/100], Train Loss: 0.35530421745777113, Val Loss: 0.39760219058394425, Val Acc: 0.8452500104904175\n",
      "Epoch: [47/100], Train Loss: 0.3573583954572677, Val Loss: 0.40477392750978475, Val Acc: 0.8402500152587891\n",
      "Epoch: [48/100], Train Loss: 0.3578105573654174, Val Loss: 0.4409258131384848, Val Acc: 0.8317499756813049\n",
      "Epoch: [49/100], Train Loss: 0.33756655019521714, Val Loss: 0.37652272647619245, Val Acc: 0.8495000004768372\n",
      "Epoch: [50/100], Train Loss: 0.3429216388463974, Val Loss: 0.3831123657226563, Val Acc: 0.8514999747276306\n",
      "Epoch: [51/100], Train Loss: 0.3349141245484351, Val Loss: 0.40441955888271325, Val Acc: 0.8424999713897705\n",
      "Epoch: [52/100], Train Loss: 0.3299002396464349, Val Loss: 0.37595928478240964, Val Acc: 0.8557500243186951\n",
      "Epoch: [53/100], Train Loss: 0.3299750483632088, Val Loss: 0.3900992174148559, Val Acc: 0.8495000004768372\n",
      "Epoch: [54/100], Train Loss: 0.32108694034814844, Val Loss: 0.37098124730587023, Val Acc: 0.8547499775886536\n",
      "Epoch: [55/100], Train Loss: 0.3309939012527467, Val Loss: 0.40370045608282085, Val Acc: 0.8452500104904175\n",
      "Epoch: [56/100], Train Loss: 0.325013067007065, Val Loss: 0.36086271679401405, Val Acc: 0.8542500138282776\n",
      "Epoch: [57/100], Train Loss: 0.3218055998981, Val Loss: 0.35629945701360705, Val Acc: 0.8627499938011169\n",
      "Epoch: [58/100], Train Loss: 0.31347644603252395, Val Loss: 0.3440078646242619, Val Acc: 0.8622499704360962\n",
      "Epoch: [59/100], Train Loss: 0.3102866138219833, Val Loss: 0.3600490390956401, Val Acc: 0.8662499785423279\n",
      "Epoch: [60/100], Train Loss: 0.3260119906663894, Val Loss: 0.37261414331197756, Val Acc: 0.8552500009536743\n",
      "Epoch: [61/100], Train Loss: 0.30500430214405044, Val Loss: 0.34476535205543046, Val Acc: 0.8682500123977661\n",
      "Epoch: [62/100], Train Loss: 0.30492216080427176, Val Loss: 0.3542511784136296, Val Acc: 0.8652499914169312\n",
      "Epoch: [63/100], Train Loss: 0.3035252994298934, Val Loss: 0.36047431844472894, Val Acc: 0.8572499752044678\n",
      "Epoch: [64/100], Train Loss: 0.3056192950606346, Val Loss: 0.3348714162409306, Val Acc: 0.8727499842643738\n",
      "Epoch: [65/100], Train Loss: 0.3067516168355942, Val Loss: 0.365745565533638, Val Acc: 0.862500011920929\n",
      "Epoch: [66/100], Train Loss: 0.2968422434329987, Val Loss: 0.34402206879854214, Val Acc: 0.8647500276565552\n",
      "Epoch: [67/100], Train Loss: 0.2932056080698967, Val Loss: 0.35921169918775553, Val Acc: 0.8637499809265137\n",
      "Epoch: [68/100], Train Loss: 0.2934738184809686, Val Loss: 0.34244863194227204, Val Acc: 0.8722500205039978\n",
      "Epoch: [69/100], Train Loss: 0.2912324131727218, Val Loss: 0.35500622600316994, Val Acc: 0.8647500276565552\n",
      "Epoch: [70/100], Train Loss: 0.3106499292254446, Val Loss: 0.37905105960369123, Val Acc: 0.8560000061988831\n",
      "Epoch: [71/100], Train Loss: 0.2918353402614593, Val Loss: 0.343638674736023, Val Acc: 0.8730000257492065\n",
      "Epoch: [72/100], Train Loss: 0.29517490375041944, Val Loss: 0.3510416780412198, Val Acc: 0.8725000023841858\n",
      "Epoch: [73/100], Train Loss: 0.2866927154362202, Val Loss: 0.35386432951688773, Val Acc: 0.8647500276565552\n",
      "Epoch: [74/100], Train Loss: 0.28988092839717866, Val Loss: 0.35846886008977885, Val Acc: 0.8712499737739563\n",
      "Epoch: [75/100], Train Loss: 0.2805824618339538, Val Loss: 0.3604865256547929, Val Acc: 0.8634999990463257\n",
      "Epoch: [76/100], Train Loss: 0.28077200385928136, Val Loss: 0.35183539366722105, Val Acc: 0.8702499866485596\n",
      "Epoch: [77/100], Train Loss: 0.2834493144154548, Val Loss: 0.370819000005722, Val Acc: 0.8667500019073486\n",
      "Epoch: [78/100], Train Loss: 0.27683748549222936, Val Loss: 0.34301781964302064, Val Acc: 0.8722500205039978\n",
      "Epoch: [79/100], Train Loss: 0.2814852833747864, Val Loss: 0.3437073455452919, Val Acc: 0.8740000128746033\n",
      "Epoch: [80/100], Train Loss: 0.277704749017954, Val Loss: 0.3364271702170371, Val Acc: 0.8742499947547913\n",
      "Epoch: [81/100], Train Loss: 0.28241474032402036, Val Loss: 0.3320867578983307, Val Acc: 0.8787500262260437\n",
      "Epoch: [82/100], Train Loss: 0.26857642954587924, Val Loss: 0.34232515436410904, Val Acc: 0.8694999814033508\n",
      "Epoch: [83/100], Train Loss: 0.28301797389984135, Val Loss: 0.3515910689830779, Val Acc: 0.8697500228881836\n",
      "Epoch: [84/100], Train Loss: 0.26378983032703385, Val Loss: 0.3285429388284683, Val Acc: 0.8759999871253967\n",
      "Epoch: [85/100], Train Loss: 0.26176940968632695, Val Loss: 0.3336341570317745, Val Acc: 0.8742499947547913\n",
      "Epoch: [86/100], Train Loss: 0.26173860025405893, Val Loss: 0.3223735877871514, Val Acc: 0.8790000081062317\n",
      "Epoch: [87/100], Train Loss: 0.28039353263378136, Val Loss: 0.3798067974746227, Val Acc: 0.8634999990463257\n",
      "Epoch: [88/100], Train Loss: 0.2636406870484353, Val Loss: 0.3560351384878157, Val Acc: 0.8694999814033508\n",
      "Epoch: [89/100], Train Loss: 0.26378181362152114, Val Loss: 0.3526613119840622, Val Acc: 0.8725000023841858\n",
      "Epoch: [90/100], Train Loss: 0.2501122013628484, Val Loss: 0.3157339957356453, Val Acc: 0.8822500109672546\n",
      "Epoch: [91/100], Train Loss: 0.257970635831356, Val Loss: 0.3381233482956886, Val Acc: 0.8709999918937683\n",
      "Epoch: [92/100], Train Loss: 0.24602998766303066, Val Loss: 0.3094585692286491, Val Acc: 0.8862500190734863\n",
      "Epoch: [93/100], Train Loss: 0.24785699731111532, Val Loss: 0.3430275917351245, Val Acc: 0.8807500004768372\n",
      "Epoch: [94/100], Train Loss: 0.24117681667208676, Val Loss: 0.3203397606611252, Val Acc: 0.8877500295639038\n",
      "Epoch: [95/100], Train Loss: 0.24175404679775236, Val Loss: 0.3142960512638092, Val Acc: 0.8817499876022339\n",
      "Epoch: [96/100], Train Loss: 0.24424209532141689, Val Loss: 0.32027433383464826, Val Acc: 0.8914999961853027\n",
      "Epoch: [97/100], Train Loss: 0.23351699045300484, Val Loss: 0.3212618013620377, Val Acc: 0.8877500295639038\n",
      "Epoch: [98/100], Train Loss: 0.2326453175842761, Val Loss: 0.32536442351341244, Val Acc: 0.8872500061988831\n",
      "Epoch: [99/100], Train Loss: 0.2324571645855902, Val Loss: 0.3123038637042045, Val Acc: 0.8884999752044678\n",
      "Epoch: [100/100], Train Loss: 0.23097358167171464, Val Loss: 0.3249983851313591, Val Acc: 0.8897500038146973\n",
      "k = 5\n",
      "Epoch: [1/100], Train Loss: 1.0464001045227056, Val Loss: 1.0424155855178832, Val Acc: 0.5292500257492065\n",
      "Epoch: [2/100], Train Loss: 0.8563938772678381, Val Loss: 0.7802917561531068, Val Acc: 0.684249997138977\n",
      "Epoch: [3/100], Train Loss: 0.8257840294837949, Val Loss: 0.747818971633911, Val Acc: 0.7005000114440918\n",
      "Epoch: [4/100], Train Loss: 0.8055204801559447, Val Loss: 0.7255982532501222, Val Acc: 0.7064999938011169\n",
      "Epoch: [5/100], Train Loss: 0.7822946052551266, Val Loss: 0.6949649636745453, Val Acc: 0.7055000066757202\n",
      "Epoch: [6/100], Train Loss: 0.7496992003917691, Val Loss: 0.6734922184944152, Val Acc: 0.7225000262260437\n",
      "Epoch: [7/100], Train Loss: 0.7086990623474121, Val Loss: 0.6737531602382659, Val Acc: 0.7257500290870667\n",
      "Epoch: [8/100], Train Loss: 0.7084324445724486, Val Loss: 0.6967540831565857, Val Acc: 0.7009999752044678\n",
      "Epoch: [9/100], Train Loss: 0.6689285649061205, Val Loss: 0.6269287543296815, Val Acc: 0.7517499923706055\n",
      "Epoch: [10/100], Train Loss: 0.6515214151144031, Val Loss: 0.617736929178238, Val Acc: 0.7534999847412109\n",
      "Epoch: [11/100], Train Loss: 0.624160931110382, Val Loss: 0.6139555189609529, Val Acc: 0.7549999952316284\n",
      "Epoch: [12/100], Train Loss: 0.6125371735095979, Val Loss: 0.5935750167369841, Val Acc: 0.7689999938011169\n",
      "Epoch: [13/100], Train Loss: 0.5881003187894822, Val Loss: 0.5953055577278137, Val Acc: 0.7667499780654907\n",
      "Epoch: [14/100], Train Loss: 0.5838490597009659, Val Loss: 0.59333957695961, Val Acc: 0.7682499885559082\n",
      "Epoch: [15/100], Train Loss: 0.572680497765541, Val Loss: 0.5984649194478988, Val Acc: 0.7662500143051147\n",
      "Epoch: [16/100], Train Loss: 0.5566318781375886, Val Loss: 0.5763611344099046, Val Acc: 0.777999997138977\n",
      "Epoch: [17/100], Train Loss: 0.5448415826559064, Val Loss: 0.5560837655067444, Val Acc: 0.7829999923706055\n",
      "Epoch: [18/100], Train Loss: 0.5533105959892272, Val Loss: 0.5354469263553618, Val Acc: 0.796750009059906\n",
      "Epoch: [19/100], Train Loss: 0.5314646178483959, Val Loss: 0.5687898776531221, Val Acc: 0.7787500023841858\n",
      "Epoch: [20/100], Train Loss: 0.5232812330722809, Val Loss: 0.5435823338031769, Val Acc: 0.7870000004768372\n",
      "Epoch: [21/100], Train Loss: 0.5214965697526932, Val Loss: 0.5888948690891265, Val Acc: 0.7680000066757202\n",
      "Epoch: [22/100], Train Loss: 0.5032466232776642, Val Loss: 0.5178202381134032, Val Acc: 0.8029999732971191\n",
      "Epoch: [23/100], Train Loss: 0.4966996849775312, Val Loss: 0.5165198835134506, Val Acc: 0.8054999709129333\n",
      "Epoch: [24/100], Train Loss: 0.4921826417446138, Val Loss: 0.555414923787117, Val Acc: 0.781000018119812\n",
      "Epoch: [25/100], Train Loss: 0.476739045381546, Val Loss: 0.5198726985454559, Val Acc: 0.8012499809265137\n",
      "Epoch: [26/100], Train Loss: 0.46729409623146034, Val Loss: 0.5342086074352264, Val Acc: 0.8004999756813049\n",
      "Epoch: [27/100], Train Loss: 0.46633772957325, Val Loss: 0.49460400438308727, Val Acc: 0.8130000233650208\n",
      "Epoch: [28/100], Train Loss: 0.45463602232933015, Val Loss: 0.5069133460521699, Val Acc: 0.8147500157356262\n",
      "Epoch: [29/100], Train Loss: 0.44898253905773194, Val Loss: 0.5165346106290816, Val Acc: 0.8057500123977661\n",
      "Epoch: [30/100], Train Loss: 0.4268390867114069, Val Loss: 0.4988467016220092, Val Acc: 0.8149999976158142\n",
      "Epoch: [31/100], Train Loss: 0.408849585592747, Val Loss: 0.4656917996406555, Val Acc: 0.8322499990463257\n",
      "Epoch: [32/100], Train Loss: 0.38705043357610697, Val Loss: 0.47165107405185697, Val Acc: 0.8327500224113464\n",
      "Epoch: [33/100], Train Loss: 0.3620318765640258, Val Loss: 0.4401237818598747, Val Acc: 0.8379999995231628\n",
      "Epoch: [34/100], Train Loss: 0.3609740462303159, Val Loss: 0.43617592620849616, Val Acc: 0.8432499766349792\n",
      "Epoch: [35/100], Train Loss: 0.36440702563524235, Val Loss: 0.40887190908193577, Val Acc: 0.8585000038146973\n",
      "Epoch: [36/100], Train Loss: 0.3503470537066461, Val Loss: 0.3797355419993401, Val Acc: 0.8669999837875366\n",
      "Epoch: [37/100], Train Loss: 0.3693800937533381, Val Loss: 0.3985520011782646, Val Acc: 0.8615000247955322\n",
      "Epoch: [38/100], Train Loss: 0.34229570484161365, Val Loss: 0.3980258104205131, Val Acc: 0.862500011920929\n",
      "Epoch: [39/100], Train Loss: 0.3381548284292221, Val Loss: 0.4088945512175561, Val Acc: 0.8519999980926514\n",
      "Epoch: [40/100], Train Loss: 0.330970672607422, Val Loss: 0.3856385895013809, Val Acc: 0.8662499785423279\n",
      "Epoch: [41/100], Train Loss: 0.32768225860595723, Val Loss: 0.3831919448971748, Val Acc: 0.8657500147819519\n",
      "Epoch: [42/100], Train Loss: 0.33013556033372865, Val Loss: 0.40027259683609, Val Acc: 0.8579999804496765\n",
      "Epoch: [43/100], Train Loss: 0.3214862774014473, Val Loss: 0.39328872126340864, Val Acc: 0.8602499961853027\n",
      "Epoch: [44/100], Train Loss: 0.33909990620613084, Val Loss: 0.4035260939598085, Val Acc: 0.8600000143051147\n",
      "Epoch: [45/100], Train Loss: 0.3411309963464737, Val Loss: 0.40324972030520445, Val Acc: 0.8619999885559082\n",
      "Epoch: [46/100], Train Loss: 0.3374000980854035, Val Loss: 0.3968126341700554, Val Acc: 0.8615000247955322\n",
      "Epoch: [47/100], Train Loss: 0.3172148689031601, Val Loss: 0.3917483444809914, Val Acc: 0.8610000014305115\n",
      "Epoch: [48/100], Train Loss: 0.3102362218499186, Val Loss: 0.3864908339977264, Val Acc: 0.8627499938011169\n",
      "Epoch: [49/100], Train Loss: 0.32933234202861794, Val Loss: 0.41916751155257226, Val Acc: 0.8552500009536743\n",
      "Epoch: [50/100], Train Loss: 0.3294078090190885, Val Loss: 0.4185582479834556, Val Acc: 0.8547499775886536\n",
      "Epoch: [51/100], Train Loss: 0.30566561964154243, Val Loss: 0.40095028072595595, Val Acc: 0.8662499785423279\n",
      "Epoch: [52/100], Train Loss: 0.3136681876778604, Val Loss: 0.40056825488805775, Val Acc: 0.8647500276565552\n",
      "Epoch: [53/100], Train Loss: 0.3106960477232933, Val Loss: 0.3825532554388046, Val Acc: 0.8654999732971191\n",
      "Epoch: [54/100], Train Loss: 0.30716057795286195, Val Loss: 0.40166182559728625, Val Acc: 0.8667500019073486\n",
      "Epoch: [55/100], Train Loss: 0.3038096991777419, Val Loss: 0.39485927820205685, Val Acc: 0.8659999966621399\n",
      "Epoch: [56/100], Train Loss: 0.30852997940778704, Val Loss: 0.3914043689966202, Val Acc: 0.8602499961853027\n",
      "Epoch: [57/100], Train Loss: 0.29422444239258755, Val Loss: 0.3877260219454765, Val Acc: 0.8702499866485596\n",
      "Epoch: [58/100], Train Loss: 0.30723251420259473, Val Loss: 0.3886303555965424, Val Acc: 0.8687499761581421\n",
      "Epoch: [59/100], Train Loss: 0.29677494323253634, Val Loss: 0.3778898203372956, Val Acc: 0.8715000152587891\n",
      "Epoch: [60/100], Train Loss: 0.28716774612665175, Val Loss: 0.37398534911870956, Val Acc: 0.874750018119812\n",
      "Epoch: [61/100], Train Loss: 0.28792674994468687, Val Loss: 0.37431440177559855, Val Acc: 0.8755000233650208\n",
      "Epoch: [62/100], Train Loss: 0.2931055607199667, Val Loss: 0.40143281012773513, Val Acc: 0.8665000200271606\n",
      "Epoch: [63/100], Train Loss: 0.2959649896621703, Val Loss: 0.3719781674146652, Val Acc: 0.8712499737739563\n",
      "Epoch: [64/100], Train Loss: 0.27628554260730753, Val Loss: 0.3658828070759774, Val Acc: 0.8742499947547913\n",
      "Epoch: [65/100], Train Loss: 0.2826585059762, Val Loss: 0.3764177948236466, Val Acc: 0.8757500052452087\n",
      "Epoch: [66/100], Train Loss: 0.2778768039643764, Val Loss: 0.367984273314476, Val Acc: 0.8772500157356262\n",
      "Epoch: [67/100], Train Loss: 0.27792895191907874, Val Loss: 0.3695992420911789, Val Acc: 0.8737499713897705\n",
      "Epoch: [68/100], Train Loss: 0.28010883080959315, Val Loss: 0.40565245199203487, Val Acc: 0.8697500228881836\n",
      "Epoch: [69/100], Train Loss: 0.26832038256526, Val Loss: 0.38882896983623516, Val Acc: 0.8709999918937683\n",
      "Epoch: [70/100], Train Loss: 0.2742203337550164, Val Loss: 0.4060831988453866, Val Acc: 0.8629999756813049\n",
      "Epoch: [71/100], Train Loss: 0.29628741425275784, Val Loss: 0.42321041163802153, Val Acc: 0.8565000295639038\n",
      "Epoch: [72/100], Train Loss: 0.2616179601848127, Val Loss: 0.3729833303093911, Val Acc: 0.8715000152587891\n",
      "Epoch: [73/100], Train Loss: 0.2615599205493927, Val Loss: 0.3817713883817196, Val Acc: 0.8694999814033508\n",
      "Epoch: [74/100], Train Loss: 0.2747447032928468, Val Loss: 0.3991740927398206, Val Acc: 0.8640000224113464\n",
      "Epoch: [75/100], Train Loss: 0.2691611694097519, Val Loss: 0.37422967135906227, Val Acc: 0.8769999742507935\n",
      "Epoch: [76/100], Train Loss: 0.2564703648388385, Val Loss: 0.3763525371849537, Val Acc: 0.8705000281333923\n",
      "Epoch: [77/100], Train Loss: 0.26460389029979725, Val Loss: 0.39873034262657153, Val Acc: 0.8675000071525574\n",
      "Epoch: [78/100], Train Loss: 0.2708802076280116, Val Loss: 0.40284832018613825, Val Acc: 0.8640000224113464\n",
      "Epoch: [79/100], Train Loss: 0.27939943754673, Val Loss: 0.3784852438867094, Val Acc: 0.8647500276565552\n",
      "Epoch: [80/100], Train Loss: 0.2522404283881188, Val Loss: 0.3725475648641587, Val Acc: 0.8755000233650208\n",
      "Epoch: [81/100], Train Loss: 0.25619694039225577, Val Loss: 0.37351010891795167, Val Acc: 0.8762500286102295\n",
      "Epoch: [82/100], Train Loss: 0.2587753702700138, Val Loss: 0.3799609976410866, Val Acc: 0.8742499947547913\n",
      "Epoch: [83/100], Train Loss: 0.24646838623285283, Val Loss: 0.39030597877502443, Val Acc: 0.8672500252723694\n",
      "Epoch: [84/100], Train Loss: 0.24595375579595566, Val Loss: 0.37813740313053124, Val Acc: 0.8730000257492065\n",
      "Epoch: [85/100], Train Loss: 0.249180424630642, Val Loss: 0.39128568834066396, Val Acc: 0.8675000071525574\n",
      "Epoch: [86/100], Train Loss: 0.24843776148557667, Val Loss: 0.37428813904523855, Val Acc: 0.8737499713897705\n",
      "Epoch: [87/100], Train Loss: 0.23708629351854327, Val Loss: 0.36590571549534795, Val Acc: 0.8794999718666077\n",
      "Epoch: [88/100], Train Loss: 0.25090926009416586, Val Loss: 0.3756826813220977, Val Acc: 0.8767499923706055\n",
      "Epoch: [89/100], Train Loss: 0.23632818010449408, Val Loss: 0.37927089482545856, Val Acc: 0.874750018119812\n",
      "Epoch: [90/100], Train Loss: 0.23058267784118655, Val Loss: 0.3747053068280221, Val Acc: 0.8690000176429749\n",
      "Epoch: [91/100], Train Loss: 0.23015234488248823, Val Loss: 0.3704810474514962, Val Acc: 0.8805000185966492\n",
      "Epoch: [92/100], Train Loss: 0.2403326369822025, Val Loss: 0.36370880612730977, Val Acc: 0.8799999952316284\n",
      "Epoch: [93/100], Train Loss: 0.23473174610733985, Val Loss: 0.3896275812387467, Val Acc: 0.8654999732971191\n",
      "Epoch: [94/100], Train Loss: 0.22717724874615675, Val Loss: 0.38116770777106296, Val Acc: 0.8705000281333923\n",
      "Epoch: [95/100], Train Loss: 0.2307557381093501, Val Loss: 0.38309598052501675, Val Acc: 0.871999979019165\n",
      "Epoch: [96/100], Train Loss: 0.23481099423766125, Val Loss: 0.38844087404012684, Val Acc: 0.8667500019073486\n",
      "Epoch: [97/100], Train Loss: 0.21934741380810763, Val Loss: 0.38101832249760637, Val Acc: 0.8744999766349792\n",
      "Epoch: [98/100], Train Loss: 0.2207893208861352, Val Loss: 0.3815340449512005, Val Acc: 0.8742499947547913\n",
      "Epoch: [99/100], Train Loss: 0.22689025506377225, Val Loss: 0.36723972514271735, Val Acc: 0.8759999871253967\n",
      "Epoch: [100/100], Train Loss: 0.23474764603376372, Val Loss: 0.38168515706062306, Val Acc: 0.8742499947547913\n",
      "validation accuracy: 0.8742499947547913\n"
     ]
    }
   ],
   "source": [
    "val_acc_t = []\n",
    "for k in range(1,5):\n",
    "    print(\"k = {}\".format(k+1))\n",
    "    dat_health = np.zeros([64*2*50, 81])\n",
    "    for i in range(64):\n",
    "        t = str(train_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + '/Data/Z_Healthy/c_Z'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_health[i*50:(i+1)*50,:]=a\n",
    "    for i in range(64):\n",
    "        t = str(train_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + '/Data/O_Healthy/c_O'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_health[(i+64)*50:(i+65)*50,:]=a\n",
    "    dat_inter = np.zeros([64*2*50, 81])\n",
    "    for i in range(64):\n",
    "        t = str(train_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + '/Data/F_Interictal/c_F'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_inter[i*50:(i+1)*50,:]=a\n",
    "    for i in range(64):\n",
    "        t = str(train_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + 't/Data/N_Interictal/c_N'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_inter[(i+64)*50:(i+65)*50,:]=a\n",
    "    dat_ict = np.zeros([64*50, 81])\n",
    "    for i in range(64):\n",
    "        t = str(train_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + '/Data/S_Ictal/c_S'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_ict[i*50:(i+1)*50,:]=a\n",
    "    dat_health_t = np.zeros([32*50, 81])\n",
    "    for i in range(16):\n",
    "        t = str(val_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + '/Data/Z_Healthy/c_Z'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_health_t[i*50:(i+1)*50,:]=a\n",
    "    for i in range(16):\n",
    "        t = str(val_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path +'/Data/O_Healthy/c_O'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_health_t[(i+16)*50:(i+17)*50,:]=a\n",
    "    dat_inter_t = np.zeros([32*50, 81])\n",
    "    for i in range(16):\n",
    "        t = str(val_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + '/Data/F_Interictal/c_F'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_inter_t[i*50:(i+1)*50,:]=a\n",
    "    for i in range(16):\n",
    "        t = str(val_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + '/Data/N_Interictal/c_N'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_inter_t[(i+16)*50:(i+17)*50,:]=a\n",
    "    dat_ict_t = np.zeros([16*50, 81])\n",
    "    for i in range(16):\n",
    "        t = str(val_idx[k][i]).zfill(3)\n",
    "        a = np.loadtxt(path + '/Data/S_Ictal/c_S'+t+'.txt')\n",
    "        a = a[:50*81].reshape([50, 81])\n",
    "        dat_ict_t[i*50:(i+1)*50,:]=a\n",
    "    \n",
    "    dat_health = np.expand_dims(dat_health, 2)\n",
    "    dat_inter = np.expand_dims(dat_inter, 2)\n",
    "    dat_ict = np.expand_dims(dat_ict, 2)\n",
    "    dat_health_t = np.expand_dims(dat_health_t, 2)\n",
    "    dat_inter_t = np.expand_dims(dat_inter_t, 2)\n",
    "    dat_ict_t = np.expand_dims(dat_ict_t, 2)\n",
    "    \n",
    "    dat_agg = np.concatenate([dat_health, dat_inter, dat_ict])\n",
    "    dat_lab = np.concatenate([[0]*128*50, [1]*128*50, [2]*64*50])\n",
    "    dat_agg_t = np.concatenate([dat_health_t, dat_inter_t, dat_ict_t])\n",
    "    dat_lab_t = np.concatenate([[0]*32*50, [1]*32*50, [2]*16*50])\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    train_dataset = MyDataset(dat_agg, dat_lab)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                collate_fn=newsgroup_collate_func,\n",
    "                                shuffle=True)\n",
    "    val_dataset = MyDataset(dat_agg_t, dat_lab_t)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                collate_fn=newsgroup_collate_func,\n",
    "                                shuffle=False)\n",
    "    \n",
    "    hidden_size = 32\n",
    "    num_layers = 1\n",
    "    learning_rate = 0.001\n",
    "    model = RNN(hidden_size, num_layers).to(device)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    num_epochs = 100\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "    #scheduler.step()\n",
    "        loss = do_train(\n",
    "            model=model, \n",
    "            criterion=criterion,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "        val_loss, val_acc = acc(model, val_loader, criterion)\n",
    "        if(val_acc > best_acc):\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pkl\")\n",
    "        train_loss, train_acc = acc(model,train_loader, criterion)\n",
    "        print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "    model.load_state_dict(torch.load(\"best_model.pkl\"))\n",
    "    t_dataset = MyDataset(dat_agg_t, dat_lab_t)\n",
    "    t_loader = torch.utils.data.DataLoader(dataset=t_dataset,\n",
    "                                batch_size=50,\n",
    "                                collate_fn=newsgroup_collate_func,\n",
    "                                shuffle=False)\n",
    "    pred_lab = []\n",
    "    for x, y in t_loader:\n",
    "        y_hat = model(x)\n",
    "        pred = torch.exp(y_hat).max(1)[1].numpy()\n",
    "        pred_lab.append(np.argmax([len(pred[pred==0]), len(pred[pred==1]), len(pred[pred==2])]))\n",
    "    true_lab = np.concatenate([[0]*32,[1]*32,[2]*16])\n",
    "    val_acc_t.append(len(true_lab[np.array(pred_lab)==true_lab])/80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.9574999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"validation accuracy: {}\".format(np.mean(val_acc_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat_health_test = np.zeros([40*50, 81])\n",
    "for i in range(20):\n",
    "    t = str(test_idx[i]).zfill(3)\n",
    "    a = np.loadtxt(path + '/Data/Z_Healthy/c_Z'+t+'.txt')\n",
    "    a = a[:50*81].reshape([50, 81])\n",
    "    dat_health_test[i*50:(i+1)*50,:]=a\n",
    "for i in range(20):\n",
    "    t = str(test_idx[i]).zfill(3)\n",
    "    a = np.loadtxt(path + '/Data/O_Healthy/c_O'+t+'.txt')\n",
    "    a = a[:50*81].reshape([50, 81])\n",
    "    dat_health_test[(i+20)*50:(i+21)*50,:]=a\n",
    "dat_inter_test = np.zeros([40*50, 81])\n",
    "for i in range(20):\n",
    "    t = str(test_idx[i]).zfill(3)\n",
    "    a = np.loadtxt(path + '/Data/F_Interictal/c_F'+t+'.txt')\n",
    "    a = a[:50*81].reshape([50, 81])\n",
    "    dat_inter_test[i*50:(i+1)*50,:]=a\n",
    "for i in range(20):\n",
    "    t = str(test_idx[i]).zfill(3)\n",
    "    a = np.loadtxt(path + '/Data/N_Interictal/c_N'+t+'.txt')\n",
    "    a = a[:50*81].reshape([50, 81])\n",
    "    dat_inter_test[(i+20)*50:(i+21)*50,:]=a\n",
    "dat_ict_test = np.zeros([20*50, 81])\n",
    "for i in range(20):\n",
    "    t = str(test_idx[i]).zfill(3)\n",
    "    a = np.loadtxt(path + '/Data/S_Ictal/c_S'+t+'.txt')\n",
    "    a = a[:50*81].reshape([50, 81])\n",
    "    dat_ict_test[i*50:(i+1)*50,:]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat_health_test = np.expand_dims(dat_health_test, 2)\n",
    "dat_inter_test = np.expand_dims(dat_inter_test, 2)\n",
    "dat_ict_test = np.expand_dims(dat_ict_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat_agg_test = np.concatenate([dat_health_test, dat_inter_test, dat_ict_test])\n",
    "dat_lab_test = np.concatenate([[0]*40*50, [1]*40*50, [2]*20*50])\n",
    "test_dataset = MyDataset(dat_agg_test, dat_lab_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=50,\n",
    "                                collate_fn=newsgroup_collate_func,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "pred_lab = []\n",
    "for x, y in test_loader:\n",
    "    y_hat = model(x)\n",
    "    pred = torch.exp(y_hat).max(1)[1].numpy()\n",
    "    pred_lab.append(np.argmax([len(pred[pred==0]), len(pred[pred==1]), len(pred[pred==2])]))\n",
    "true_lab = np.concatenate([[0]*40,[1]*40,[2]*20])\n",
    "print(\"test accuracy: {}\".format(len(true_lab[np.array(pred_lab)==true_lab])/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
